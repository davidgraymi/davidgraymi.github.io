---
title: "Sign Language Interpretation System"
link: "https://github.com/davidgraymi/Sign-Language-Interpretation-System"
type: "School"
---

In this project myself and four other seniors in the Computer Science program at MSU created a system that allows a user to control any smart technology (like a lightbulb) with American Sign Language by signing in front of a camera. Our system contains 3 modules, hand/landmark tracking, gesture classification, and grammer correction, and utilizes a client-server connection to prove that this software can be implemented and accessed by any smart device with a camera. The biggest task I dealt with was building, training, and testing the gesture classifier. For this task I built a convolutional neural network that classifies 14 hand gestures with 98.8% accuracy. For the source code, live demonstrations, papers, and more check out my github.
